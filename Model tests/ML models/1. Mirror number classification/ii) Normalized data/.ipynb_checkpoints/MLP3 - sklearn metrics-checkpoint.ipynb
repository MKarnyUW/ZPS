{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf420a35",
   "metadata": {},
   "source": [
    "# I. Mirror number classification\n",
    "### 1) MLP (multilayer perceptron) network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150a509",
   "metadata": {},
   "source": [
    "*Based on: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/*\n",
    "\n",
    "Here we implement our first ML model for the basic task of classifying the number of mirrors in the system, based on the interference pattern. We start with a simple deep neural network - an MLP - which consists of several fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47014f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3da4ca",
   "metadata": {},
   "source": [
    "The input data is in the form of a CSV file, where each row has numbers separated by commas. The first value signifies the number of mirrors present in the system; subsequent numbers are the values of total transmittance for equally spaced wavenumbers k. The number of k values is constant through all the rows and depends on the generated data - the more k values, the more dense the probing of the k-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fcf52",
   "metadata": {},
   "source": [
    "Let us load the dataset and visualize some interference pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cba9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loadtxt('data50_2.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "# number of k samples\n",
    "k_samples = 50\n",
    "\n",
    "# number of classes (of mirror number)\n",
    "num_classes = 8\n",
    "\n",
    "# list of class labels\n",
    "classes = [f'{class_id}' for class_id in range(3, 11)]\n",
    "\n",
    "# mirror numbers (the correct classes) in the first column\n",
    "y = dataset[:,0]\n",
    "\n",
    "# convenient to have the classes starting from 0, so just subtract 3 from every y\n",
    "y -= 3\n",
    "\n",
    "# transmittance values\n",
    "X = dataset[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f6896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuvUlEQVR4nO3de3zcdZno8c8zM5kkM2nSpElvSUNKW0rvCAWKXQUpSEEoy6oLqCt1L/XG0T267sHdPaDsWY/u7bgqL1dkUXRBREUpWCiKKGClUEqhl1B6odCk6TX3mWSu3/PHzG86tLlMJr/5/WbS5/165dW5/DLzTdo+efJ8v9/nK8YYlFJKlT6P2wNQSillDw3oSik1QWhAV0qpCUIDulJKTRAa0JVSaoLwufXG9fX1pqWlxa23V0qpkvTSSy8dN8Y0DPWcawG9paWFLVu2uPX2SilVkkTkzeGe05KLUkpNEBrQlVJqgtCArpRSE4RrNfShxGIx2traGBwcdHsoE1pFRQVNTU2UlZW5PRSllI2KKqC3tbUxadIkWlpaEBG3hzMhGWM4ceIEbW1tzJ492+3hKKVsVFQll8HBQaZMmaLBvIBEhClTpuhvQUpNQEUV0AEN5g7Q77FSE1PRBXSlVPHatPc4uw71uj0MNQwN6KdoaWlhyZIlnHfeeSxfvnzIa4wxfOYzn2Hu3LksXbqUrVu35v1+69ev56tf/Wren6+Uk/7hFzv43EPb0HMUilNRTYoWi6effpr6+vphn3/88cfZs2cPe/bsYfPmzXzyk59k8+bNeb3XmjVrWLNmzWmPx+NxfD7fsPeHk+t1SuWjdzDO8f4IW9/q4oKz6twejjqF/s/PwyOPPMJHP/pRRIQVK1bQ3d1NR0cHM2bMyFxz4MABVq9ezYoVK9i0aRMXXnghH/vYx7jjjjs4evQo999/PxdddBHf//732bJlC9/61rdYu3YtFRUVvPzyy6xcuZLOzs633f/oRz/KJz7xCcLhMHPmzOHee++ltraWyy67jPPOO4/nnnuOm2++mebmZr785S/j9XqpqanhmWeecfG7pSaScDQOwP3Pv6UBvQgVbUD/8qM7ba/VLZxZzR3XLRrxGhHhve99LyLCxz/+cdatW3faNe3t7cyaNStzv6mpifb29rcFdIC9e/fyk5/8hHvvvZcLL7yQBx54gOeee47169fzla98hV/84henvXZbWxubNm3C6/Wydu3at91funQp3/zmN7n00ku5/fbb+fKXv8zXv/51AKLRaKY3zpIlS9i4cSONjY10d3eP7Zuk1DCSSUM4msDnER7b3sH/vnYhtUG/28NSWbSGfornnnuOrVu38vjjj3PXXXeNK7udPXs2S5YswePxsGjRIlatWoWIsGTJEg4cODDk53zwgx/E6/Wedr+np4fu7m4uvfRSAG655Za3je3GG2/M3F65ciVr167lu9/9LolEIu/xK5UtHEv9W1pz3kyi8SQ/29rm8ojUqYo2Qx8tky6UxsZGAKZOncoNN9zACy+8wLvf/e7Trjl48GDmfltbW+bzspWXl2duezyezH2Px0M8Hh/y/YPB4Ij3h5N93X/+53+yefNmfvnLX3LBBRfw0ksvMWXKlJxeR6nhhCOpf7PnN9fy1okw929+iz9fORuPR5fBFgvN0LOEQiH6+voyt5988kkWL1582nVr1qzhBz/4AcYYnn/+eWpqak4rt9itpqaG2tpann32WQB++MMfZrL1U+3bt4+LL76YO++8k4aGhrf98FEqX6FoKkMPlnv5yIqzeON4iD/sP+HyqFS2os3Q3XDkyBFuuOEGILVa5EMf+hCrV68GUlkvwCc+8QmuueYaNmzYwNy5cwkEAnzve99zZHz33XdfZlL07LPPHvZ9v/CFL7Bnzx6MMaxatYply5Y5Mj41sYXSGXrA7+PScxqofbSM+ze/ycq5w68IU84St9aTLl++3Jx6wEVraysLFixwZTxnGv1eq7F64Y1O/vQ7f+C//+Ji/mhePV/Z0Mq9z73BptsuZ2p1hdvDO2OIyEvGmCE3yWjJRSmVk1B6yWKgPDVpf/NFzcSThoe2aEmvWGhAV0rlxCq5BP2pSu3s+iB/NLeeH71wkERSd44Wg6IL6LqluPD0e6zyEY6cnBS1fPjiZtq7B/jt7qNuDUtlKaqAXlFRwYkTJzTgFJDVD72iQmueamyskouVoQNcsXAaUyeVc//mt9walspSVKtcmpqaaGtr49ixY24PZUKzTixSaizC6WWLgawMvczr4aYLZ/HNp/fS1hWmqTbg1vAURRbQy8rK9BQdpYpUKBLH5xH83rf/Yn/jRc186+m9PPjCQf7mqvkujU5BkZVclFLFKxxNEPB7TzsgpXFyJe+a18CG7R0ujUxZNKArpXLSH4kTLB/6l/pZdZX0DMQcHpE6lQZ0pVROwtE4Ab93yOeCfh/9kaH7EynnaEBXSuUkFElQNUyGHvD7iMSTxBNJh0elsuUU0EVktYjsFpG9InLbEM+vFZFjIrIt/fGX9g9VKeWmVIY+dEC31qZbLXaVO0Zd5SIiXuAu4EqgDXhRRNYbY3adcumPjTG3FmCMSqkiEIokmDm5bMjnrNp6KBKnumLoa1Th5ZKhXwTsNcbsN8ZEgQeB6ws7LKVUsRkpQ7dq66GIZuhuyiWgNwLZ3Xfa0o+d6v0i8qqI/FREZg3xPCKyTkS2iMgW3TykVGkJRRNv2/afzaqtW2eOKnfYNSn6KNBijFkK/Aq4b6iLjDF3G2OWG2OWNzQ02PTWSiknhCIjZeipx3Wli7tyCejtQHbG3ZR+LMMYc8IYE0nfvQe4wJ7hKaWKgXVA9HDr0DOTolpycVUuAf1FYJ6IzBYRP3ATsD77AhHJPn9tDdBq3xCVUm4bSK9eCQ6zDt3K0ENacnHVqKtcjDFxEbkV2Ah4gXuNMTtF5E5gizFmPfAZEVkDxIFOYG0Bx6yUctjJwy2GDhlVmVUumqG7KafmXMaYDcCGUx67Pev2F4Ev2js0pVSxyPRCHy5Dt0oumqG7SneKKqVGlcnQh5sULdNli8VAA7pSalShIU4ryubzeqgo82gN3WUa0JVSo8qcVjRMDR1SDbpCumzRVRrQlVKjOllDHz6gB8q9mVONlDs0oCulRnWyhj50yQU0Qy8GGtCVUqMKR3IouZT7tIbuMg3oSqlRhawDokfI0AN+r65ycZkGdKXUqMLROF6PUO4bPmRUlft0HbrLNKArpUYVigx9QHS2gN+nGbrLNKArpUYVisSHPX7OEiz3ag3dZRrQlVKjCkcTI9bPITUpqt0W3aUBXSk1qlA0PuIKF0j1eYkmkkTjelC0WzSgK6VGFY6MnqFbfV50YtQ9GtCVUqMKReMj7hKFrBa6ulvUNRrQlVKjCkcTw/ZCt1gtdHW3qHs0oCulRtUfiVM1TKdFi5XBa0B3jwZ0pdSowiMcEG2xJk21QZd7NKArpUaUTBrCscSwpxVZrEnTfs3QXaMBXSk1osF4AmOGP0/UcjJD14DuFg3oSqkRhUY5T9RiPa/b/92jAV0pNaLwKOeJWqwMXSdF3aMBXSk1ov5ML/SRM/RK66BonRR1jQZ0pdSIrFUro23993iEgN+bOQxDOU8DulJqRFYJZbSSC+ipRW7TgK6UGtHJDH3kkgukJkZ1UtQ9GtCVUiOyMvTRerlAKovXZYvuySmgi8hqEdktIntF5LYRrnu/iBgRWW7fEJVSbgrncJ6oparcpxuLXDRqQBcRL3AXcDWwELhZRBYOcd0k4LPAZrsHqZRyj1UTH21SFFINunTrv3tyydAvAvYaY/YbY6LAg8D1Q1z3j8DXgEEbx6eUclkoMvoB0Zag36fr0F2US0BvBA5m3W9LP5YhIucDs4wxvxzphURknYhsEZEtx44dG/NglVLOy+WAaEtQM3RXjXtSVEQ8wL8Dnx/tWmPM3caY5caY5Q0NDeN9a6WUA8I5HG5hCfi1hu6mXAJ6OzAr635T+jHLJGAx8FsROQCsANbrxKhSE0MomsgcXjEaK0M3xhR4VGoouQT0F4F5IjJbRPzATcB660ljTI8xpt4Y02KMaQGeB9YYY7YUZMRKKUeFI7ln6MFyH4mkIaIHRbti1IBujIkDtwIbgVbgIWPMThG5U0TWFHqASil3haKjHxBt0VOL3JXTj11jzAZgwymP3T7MtZeNf1hKqWIRisSZXl2R07VW4A9HE0wp5KDUkHSnqFJqRLkcEG2pslro6m5RV2hAV0qNKBSJj3q4hSWgPdFdpQFdKTWicDSRU6dF0FOL3KYBXSk1LGMMoWg8p06LoOeKuk0DulJqWIOxZOqA6Jwz9NR1/Zqhu0IDulJqWCcbc+VaQ7dWuWiG7gYN6EqpYY2lF3r2dVpDd4cGdKUcMBhL0N494PYwxswKzLlm6BVlHjyiq1zcogFdKQd87YnXuPYbz5ZcjxOrdJJrDV1EUi10teTiCg3oShVYPJFk/bZDdIVjJddaNjSG80QtgXIvYS25uEIDulIF9of9JzgRigLQmf6zVIQjY8vQIbV0sV8zdFdoQFeqwNZvO5S53R2OuTiSsctk6GMJ6H5f5geBcpYGdKUKKBJP8MTOw8yuDwLQGS6xDH2MyxYh1aArVGKlpYlCA7pSBfS73cfoG4zzZyvOAqCrxEou1ulDuRwQbakq13NF3aIBXakCevTVDuqCfq5dNgMoxRp6Ao+Q0wHRlkC5r+QmfycKDehKFUg4GufXu45w9eLpTAmW4xHoKrGSSyh9nmguB0Rbgn6vZugu0YCuVIH8uvUoA7EE1y2bidcjTA74SzJDz/U8UUtQSy6u0YCuVIE8+sohplWXc2FLHQC1gbKSzdDHIuj3Eo4lSCZLaxPVRKABXakC6BmI8bvdx7h2aSo7B6gLlmCGHh17hh4o92EMDMa1ju40DehKFcDGnYeJJpJct2xm5rHagL/k1qH3R/LI0Mu1QZdbNKArVQCPvnKI5roAy5pqMo/VlmINPRof05JFyD61SOvoTtOArpTNjvdH2LTvBNctm/G21SG1QT9d4WhJNegKRxIEcjxP1GK1CdAGXc7TgK6UzR7f3kEiad5WbgGoC5YRS5jMZp1SkM+kaFXmGDotuThNA7pSNnv0lQ7mTa1i/rRJb3u8NuAHoCtUOnX0fJYtWteX0g+uiUIDulI26ugZ4IUDnVy3bOZpm3HqgqmAXir9XDIHRI952WI6Q9dJUcdpQFfKRo+90gFwWrkFUjV0KJ1+LpF4kqQZWx8XONnIS2vozsspoIvIahHZLSJ7ReS2IZ7/hIhsF5FtIvKciCy0f6hKFb/fvn6Uc6dPynRXzFZnlVxKJEM/2ZhrjDtFM+eKakB32qgBXUS8wF3A1cBC4OYhAvYDxpglxpjzgH8G/t3ugSpVCo73RTlrSmDI56wMvVSWLlolk7EcbgEna+g6Keq8XDL0i4C9xpj9xpgo8CBwffYFxpjerLtBoHTWZSllo85wNDP5earqCh9ej5RMhm6VTIJjXLZY7vNS5hXN0F2Qy4/eRuBg1v024OJTLxKRTwOfA/zA5UO9kIisA9YBNDc3j3WsShU1Ywzd4WgmEz+ViFAbKKOzRFa5ZA6IHmMNHVJZvQZ059k2KWqMucsYMwf4X8A/DHPN3caY5caY5Q0NDXa9tVJFoT8SJ5Yw1AbKhr2mNuAvmUlRa+v+WDN063P01CLn5RLQ24FZWfeb0o8N50Hgj8cxJqVKktWnZfIwJRdI1dFLZdniyePn8sjQy32Zz1fOySWgvwjME5HZIuIHbgLWZ18gIvOy7r4P2GPfEJUqDVZtvG6EgF5Xkhn62AN6sNxHv65Dd9yof1PGmLiI3ApsBLzAvcaYnSJyJ7DFGLMeuFVErgBiQBdwSyEHrVQxslav1AZHKLkE/XS9WSIBPVNDz6/kEtYauuNy+tFrjNkAbDjlsduzbn/W5nEpVXKskstwq1wg1c+lKxzDGDOmY93cMJ4MPeD30RUesHtIahS6U1Qpm2Qy9JFq6AE/iaShd7D4s9dwNI4IVJSNPUxUleu5om7QgK6UTbrDUTwC1ZXDl1zqSmj7fyiSGPMB0RadFHWHBnSlbNIZjlJTWZY5cm4oVvZeCitdwtH4mHuhW4J+r55Y5AIN6ErZpCscG3ZTkaWUGnSFoolMb/OxCpb7GIglSOhB0Y7SgK6UTbpCw2/7t1hLGkuhn0soEs9rhQtktdDVsoujNKArZZOucGzEXaJwckljKfRzCUXiY27MZdEGXe7QgK6UTbpHaMxlqSr3UeaVkujnEo4m8tr2DyePodNTi5ylAV0pm3SGhm/MZUk16PLTXQoZejSeV2MuONlyV08tcpYGdKVsMBBNEIknR83QIbV0sRRq6OFI/hm69Xl6apGzNKArZQNrGeJoNfTUNf7SqKFH43k15oKTDb10UtRZGtCVskFXpo/L6Bl6bbCs6DN0Y0y6hp5vQE9l6Nqgy1ka0JWyQVd49G3/llSGXtyTopF4kkTS5L1s8WQNXTN0J2lAV8oGVoCuG6HToqUumJoULeZNN1Yflvwz9PRB0bps0VEa0JWygVVyGelwC0ttwE/SQO9A8Wbp1vrxfLf+W5+nDbqcpQFdKRtYJZfJIzTmsmQadBXxxGhoHKcVAZR5Pfh9Hl3l4jAN6ErZoDsco7rCh887+n+p2lII6JHxZeiQ2lyk69CdpQFdKRvksqnIcrKfSzGXXFKZdb7NuSD1w0BLLs7SgK6UDbpy2PZvyfRzKeKli1YgzreXC6QmVLXk4iwN6ErZIBXQR6+fQ2n0RM8cP5fnskVINejS5lzO0oCulA26QqP3QrcE/F78Pk9RZ+hWyWU8GXpVuU+bczlMA7pSNhhLyUVEqAsUdz8Xa/34uDJ0v1cnRR2mAV2pcRqMJQhHEzmXXCC10qWYV7mEI6kDoivL8g/oWkN3ngZ0pcapO71LNNeSC6R2lBbz9v9QNP8Doi3Bcp+ucnGYBnSlxmksfVwstQF/0dfQx7MGHVKTorr131ka0JUap3wCel3QX9SrXPojibx3iVqCfh/ReJJYImnTqNRoNKArNU5dIavkMoYaesBPz0CMeJEGu3Bk/Bl6pie6Tow6JqeALiKrRWS3iOwVkduGeP5zIrJLRF4VkadE5Cz7h6pUcbIy9LoxZujGQE+RNugKReN5d1q06KlFzhs1oIuIF7gLuBpYCNwsIgtPuexlYLkxZinwU+Cf7R6oUsVqLJ0WLZPTK2KKdaVLOJrIuxe6JaCnFjkulwz9ImCvMWa/MSYKPAhcn32BMeZpY0w4ffd5oMneYSpVvLrCMarKffh9uVcwrY6LxdrPJRTJ//g5S5WeWuS4XP4FNgIHs+63pR8bzl8Aj49nUEqVkq5wNJNx5yqz/b9IV7qkjp8bZ4aupxY5bnw/gk8hIh8BlgOXDvP8OmAdQHNzs51vrZRrxrJL1GJl6N1FWnIJReLj2vYPJ0870qWLzsklQ28HZmXdb0o/9jYicgXw98AaY0xkqBcyxtxtjFlujFne0NCQz3iVKjpdY2idaynmBl3GmNTGonHW0K3P181FzskloL8IzBOR2SLiB24C1mdfICLvAL5DKpgftX+YShWvrnBsTNv+ASr9XirLvEW5uShzQPR4M/TMuaIa0J0yakA3xsSBW4GNQCvwkDFmp4jcKSJr0pf9C1AF/EREtonI+mFeTqkJJ5+SC6Q3FxXhpGh79wAADZPKx/U61jp2XYfunJx+BBtjNgAbTnns9qzbV9g8LqVKQiyRpG8wnldArw2WFeWyxdaOXgAWzqge1+sE/JqhO013iio1DlZjrrox7BK11BZpC93Wjl68HmHu1KpxvY7XI1SW6TF0TtKArtQ4WBn2WDYVWWoDxdlCt7WjjzkNQSrG0TrXEtQGXY7SgK7UOFiTmnVjXOVifU6xZugLxllusQTLfboO3UEa0JUah5MZen4ll77BeFF1I+wOR+noGbQtoAf8Pt0p6iAN6EqNg3VIRX6rXFI/BLqL6KCLXekJUdsydL9Xe7k4SAO6UuNglUzyW+WS+pxiqqO3dvQBsGDGJFteL1ju0xq6gzSgKzUO3eEoFWUeKvPoe1JXhP1cWjt6qa/yM3VShS2vFyzXVS5O0oCu1Dh0hWNj6oOeLZOhF1lAt6vcAqkauk6KOkcDulLj0BWK5rVkEbJa6BZJySWWSLLnSL+tAT3o12WLTtKAPgEc7hkkmTRuD+OM1BWO5rVkEbIOuSiSDH3/sRDRRNK2+jmka+iROMbov08naEAvcXuP9rHya79h487Dbg/ljNQVjuW1ZBGg3Ocl6PdmVsq4rdXmFS6QCujxpCFaREszJzIN6CXuFy8fIpE0tB7uc3soZ6TxZOiQqqMXS4be2tGL3+thTsP4tvxnq0p3XCzWs1MnGg3oJcwYw/pXDgHQ1hke5Wplt0TS0DMQy7uGDundokVSQ9/V0cvcqVWUee0LC1Y/mNc6NOFwggb0EvZKWw9vdYbxCLylAd1xPQMxjGHMvdCz1QaKKUPvs7XcArB4Zg0A29t7bH1dNTQN6CVs/bZD+L0erlw4TQO6CzrH0cfFUiwZ+rG+CMf7I7ZOiALUBMo4a0qA7W0a0J2gAb1EJZKGx149xKXzG1g8s4ajfREGY7o8zEnd4+i0aEll6O7Xl+3qgT6UxY01mqE7RAN6iXrhjU6O9kVYs2wms+oCALR1aZbuJGt1Sr4biyDVz6U/EicSd/eHcSFWuFiWNtbQ3j1QVDtiJyoN6CVq/SuHCPi9XLFgWiaga9nFWVbtO99li6nPTf0wcLtBV2tHL9OrK8Z82HUuljRqHd0pGtBLUDSe5PEdHVy5cBqVfi/N6YB+sHPA5ZGdWaymWuOpoVvndrr921VqQtTe+rllkRXQ27oL8vrqJA3oJei5vcfoDsdYs2wmAPVVfirLvJqhO6wzHMXv9WQOQ87HRS11eAR+u/uYjSMbm0g8wb5j9m75z1ZTWUbLlIBm6A7QgF6C1m87RE1lGe+a1wCAiDCrrlIDusO6Q6ldoiKS92vUBv0sP6uOX7cetXFkY7PnSD/xpClYQAdY0jRZV7o4QAN6iRmIJnhy1xGuXjwdv+/kX19zXYCDGtAd1TnOXaKWVQum0trRS3u3OyWzQk6IWpY0VnOoZ5Dj/ZGCvYfSgF5ynnrtCOFoIlNusTTVpgK6NkFyTnc4Oq4JUcuqBdMAeKr1yLhfKx+tHX1UlHmYXR8s2HssaZwM6MRooWlALzGPvnKIqZPKufjsKW97vLkuQCia0KVhDuoM2ZOhz2kIMrs+6FrZpbWjl/nTJuH15F86Gs2ixlT2v0PLLgWlAb2E9A7GeHr3Md63dMZp//kyK126dKWLU7rD4+vjYhERVp07lef3naDf4cMgjDG0Hrb3UIuhVFeUcXZ9kFc1Qy8oDeglZOOOw0TjydPKLYCuRXdYMmnoHsj/tKJTrVowjWgiyXN7nF3tcrh3kO5wrOABHVI7RndoQC8oDeglZP0rh2iuC3DerMmnPTerrhJAJ0Yd0jcYJ5E0ttTQAZa31FJd4XO87OLEhKhlaVMNHT2DHOvTidFCySmgi8hqEdktIntF5LYhnn+3iGwVkbiIfMD+Yarj/RE27TvBdctmDLlMLuD3UV9VrgHdIXZsKspW5vVw2fypPP3aURIOnj7Vmm5re26BNhVlW5zeYKRZeuGMGtBFxAvcBVwNLARuFpGFp1z2FrAWeMDuAaqU3+4+RiJpeN+S08stFl2L7hyrQ2KtTSUXSC1fPBGKsu1gt22vOZpdHb001VZSXWHPbxojWTSzGhF4VSdGCyaXDP0iYK8xZr8xJgo8CFyffYEx5oAx5lVAz5kqkB3tPQT9Xs6dPnwm1VwX0IDukJOdFu0LhJedMxWvRxxdvtjaUfgJUcukijJm1wd16WIB5RLQG4GDWffb0o+NmYisE5EtIrLl2DH3tjqXou3tPSycWY1nhKVlzXUBOnoGien5jQXXmW55a1fJBVK9wy9qqeMph+roA9EEB46HHAvokOq8uL2927H3O9M4OilqjLnbGLPcGLO8oaHBybcuaYmkYdeh3kwNcjizagMkkoaO7kGHRnbmsqMX+lBWLZjK7iN9jsyF7DjUQ9LAQgfq55bFjTUc6Y1wtFf/jRZCLgG9HZiVdb8p/ZhyyBvH+xmIJTLHeQ1Hly46pzMUxesRqit8tr7uFeldo78ucNnFGMPXf/06NZVlXHJ2fUHfK9vSpsmA7hgtlFwC+ovAPBGZLSJ+4CZgfWGHpbJZ//hHy9Cbp1ibizSgF1pXOEbtOBtzDaWlPsichmDByy6/2nWE3+89weeuPIcaG+cBRmNNjGpAL4xRA7oxJg7cCmwEWoGHjDE7ReROEVkDICIXikgb8EHgOyKys5CDPtPsaO+loszDnIaRe21Mr66gzCuaoTugOxy1dYVLtisWTGPzGyfoGyzMoReReIL/88tWzplWxYcvbi7IewwnWO5jTkOVdl4skJxq6MaYDcaYc4wxc4wx/5R+7HZjzPr07ReNMU3GmKAxZooxZlEhB32m2d7ew4IZ1fi8I/91eT1C42RduuiEzlDhAvqqBdOIJQzPvH68IK//X8+9wVudYW6/dtGo/6YKYYmeMVowulO0yCWtCdFR6ueWWXUB2jSgF1x3OEZtsDClivObJ1MbKCvI8sUjvYN86zd7uXLhNP5onnO182xLGlOHmh/RiVHbaUAvcm92humPxDPnMo5mlq5Fd0RnAUsuPq+H98yfytO77d81+s9P7CaeMPz9NQtsfd2xWNJkHUmnWbrdNKAXOetXU6v96Gia6wJ0hWMFq78qONYX4UR/hBk1lQV7j1ULptEVjvGHfSdse81tB7v52dY2/vyPZtNSwN7no1k4oxqPTowWhAb0IrezvQe/18M503JbK6wHRhfe+lcOkTRwzZLpBXuPS+c3ML26gs88+DK7DvWO+/WSScOX1u+kYVI5t14+14YR5i8zMaoB3XYa0Ivc9vYezp0xibIcJ69m1epa9EJ7eGsbSxprmJfjD9l8VJX7eHDdCsp9Hj50z/Pjbmj1yCvtbDvYzd9eNZ+qcnvXzudjSVNqYlRP2LKXBvQiZoxhR3sPi3KcEIXsDF0DeiHsPtzHzkO9/Mn5eXW/GJOW+iA/XncJQb+PD333eV5t687rdUKROF99/DWWNdXw/vOb7B1knpY01nCsL8KRXm2laycN6EWsrWuA3sHcJ0Qh1Q+kusKnm4sK5OGX2/B6hOuGOGSkEJqnBHhw3QqqK8v48D2bx9yJcdPe49x49x840hvh9usWjdgLyElWT//HXj3k7kAmGA3oRezkDtGxNU/SlS6FkUgaHnn5EJed00B9Vblj7zurLsCPP34JtQE/f3bPZl56s2vUz2nt6GXt917gQ/dspisU45s3v4MLzqp1YLS5OW/WZN59TgP/71ev09Gj8z120YBexHa09+DzCPNHaJk7FG2jWxh/2HeCw72D3OBAueVUjZMr+fHHVzClys9H/2sz3/7tPp7Y0cGO9h56wrFMLbq9e4DPP/QK13zjWba+2cXfXXMuT33+Usd+o8iViPBPf7yYhDHc8YhuLLeL+7Mjalg7DvVyzrRJlPu8Y/q85roAT712lGTSFM2v2BPBwy+3ManCl2mg5bQZNZU8uO4S1n7vBb72xGtve25SuY/G2kr2Hw8B8FfvOptPXTbH9m6QdppVF+Czq87ha0+8xsadh7lqUeFWDZ0pNKAXKWtC9IoFU8f8uU11AaLxJEf7IkyvqSjA6M484WicJ3Yc5vrzZlJRNrYfsHaaXlPB4599F70DcQ52hWnrCnOwcyD1Z9cA72iezK2Xz6NxcuHWyNvpL981m0e2tfOl9TtZObe+KFbglDL97hWpjp5BOkPRMU2IWpqz2uhqQLfHxp2HCUcT3PAO91eJiAg1gTJqAjWjduAsdmVeD1/5kyW8/9ub+Lcnd3PHddoGajy0hl6kTu4QzT+g69JF+zy8tZ1ZdZUsL6KJxYni/OZaPnLxWdy36UDeSzNVigb0IrWzvQePwILpYz8ebObkCkR0c5FdDvcM8tze49xwXqPOSRTIF1bPp76qnC8+vJ24HqGYNw3oRWrHoV7mTZ1EpX/s9dpyn5cZ1RWaodvkkW3tGAM3FMmmnImouqKML61ZxM5DvXx/0wG3h1OyNKAXqe3tPTk35BrKrLqAbi6ygTGGh7e2c37zZGa72NDqTHD14ulcfu5U/u3J1zUZyZMG9CJ0tHeQY32RvCZELbq5yB67OnrZfaRPs3MHiAh3Xr8Ij8D1d/2eX77a4faQSo4G9CKU6xmiI2muC3CkN8JgLGHXsM5ID29tp8wrXLd0httDOSM01Qb4+adX0lRbyacf2Mqn7n+J4/0Tp99LW1eYf3xsF68f6SvI6+uyxSK0o70XkVTf6HxZK13augaYO7XKrqGdUaLxJI9sO8Tl504t6g06E8050ybx8CffyXee2c9//HoPz+9/hn+8fjHvK+Efqq+2dfPdZ99gw/bUbx1zp1bl3BJ7LDSgF6Edh3o4uz5IcBybLGbVpTaWHOwMa0DPw/H+CJ/6760c749w00XOHqSsUqc2ffo9c7ly4TS+8JNX+PQDW/nl9unccd0iplWXxt6KZNLwm9eO8t1n97P5jU6qyn38+coW1q6cXbCNXxrQi9CO9h4uml03rteYVad90fP1als3H//hS3SFo3zj5nfwnvlj362r7HHOtEn87JPv5O5n9/P1X+3hiR2HWd5Sx+pF07lq8fSi2xE7EE2w5c1Ofr/3BE/uOsz+YyFm1lTwD+9bwI0XzmJSRWHOobVoQC8yx/sjdPQM5nwo9HAaqsoJ+L089uohrl06gykOdgcsZT97qY0v/nw7DVXl/OyT7xxTL3pVGD6vh09dNpdrFs/g5y+3s3HnYe58bBd3PraLpU01XLVoOpfNb2BOQ5XjbRlCkTi7OnrZtPcEv993nJff6iKWMPg8wvnNtXx21TyuWTIj5wNqxkvcOjFk+fLlZsuWLa68dzH73evHuOXeF/jRX63gkjlTxvVaD77wFrc/spOaQBn/8oGlXKaZ5rDiiSRf2fAa9/7+DS45ewp3ffh86oJaNy9WbxwPsXHnYZ7YcTjTI14kdWLX3KlVzGkIMndqFS1TgtQG/dRUllFTWTamgB9LJOkZiNEzEKN3IMbhnkHeOBHiwPEQB46HeeNEiGN9kcx7L5pZzco59VwyZwoXttSNq2Q6EhF5yRizfMjnNKAXj97BGH//8x08+sohXv3Se6m24dez1o5e/vrBbew+0sfad7Zw29XnutpcqtikmqD18n8fb2XTvhN8bGULf3fNAscyKjV+HT0DvHigi31H+9l7rJ99R/vZfzxENH76jlO/z0NNZeoQGK9HsMKfFQWNMYSjCXoGYoSjQ68Qq68qZ3Z9gJYpQVrqUz84Lp5d59jE+UgBXUsuRWAwluC+TQf49u/20R2OccslZ9kSzAEWzKjmkVtX8rUnXuN7vz/A7/ce5+s3nXdGlxISScOWA508sfMwT+48Qnv3AOU+D//6wWV84AJdb15qZtRUsmbZ22vpiaShrSvMmyfCmSzbyrR7BmL0DcZJpqO5pLs5CKkblX5vJqPP/miYVE5LfbCoO0Jqhu6iWCLJQ1sO8o2n9nCkN8Kl5zTwhavmF6yD3jOvH+PzP3mFnnCMj6w4iwvOqmVxYzXNdQFEJm6Pku5wlP3HQ+w/FmLLgU5+tesIJ0JR/D4P75pbz1WLp3PFgmlaYlElYdwlFxFZDfwH4AXuMcZ89ZTny4EfABcAJ4AbjTEHRnrNMzGgR+NJDnaFOXA8xL5j/Tyw+S0OnAhzwVm1/O1V87n47PHVzHPRGYryvx/ZwZM7DxNLpP7uJ1X4WDyzhsWN1ZwzbRL1k8qpC/ipC6Y+An5v0QV8YwzRRJKecIzOcJTO/iid4ShdoSgnQlHauwbSQbyfrnAs83lV5T7ec+5UVi+azqXzG4o621JqKOMK6CLiBV4HrgTagBeBm40xu7Ku+RSw1BjzCRG5CbjBGHPjSK9bSgHdGEMiaYinP6LxJJF4Iv1nkmg8yWAsQV8kTt9gnL7B1K90vQMxegdjtHUNcOB4iINdAySSJ7/fC2ZU8zfvPYfLz53qeMCMxpO8fqSPHe09bG/vYcehXlo7eoetO9YGyqgs81KR+fCk/vR5KfN58HkEr0fe9qf1NWV/aYJgMBiT+rU4YUzm+5tIpn5rsT6iCUMsniSaSDIQTTAQSxCOxglHEoRjibd9L081dVI5ZzcEmV2fmiCbXR/k7IYqmmortT6uStp4a+gXAXuNMfvTL/YgcD2wK+ua64EvpW//FPiWiIgpQD3noRcPcvez+zP3s9/CZN3InuQwgDFgMCTT8SppUkElaQxJkw7axpBMpoONSQXvRPojH16PMKnCx8yaShbNrOHapTOZXZ+aSDm7PjX77ha/z8PixtQBCTelH4slkrR3DWQy3U7rIxylOxRjIJZgMJZgMJ5kMJqgMxRlMJYgnjj5vYonkySShljCZL73GVl3PB7BI6nvkUdSH16PUOYVyrye1IfPg98rlFs/UPw+AmVeKv1eguVeAn4fNZVlmd8k6oJ+agN+agNl+DRoqzNQLgG9ETiYdb8NuHi4a4wxcRHpAaYAx7MvEpF1wDqA5ub8dt/VBv3MP3XLrJx+U0Sybqce96RveNLPSTqgiKSCixVYPOn7Pu8pmac39We5z4vf56Hc50n/6aXc56Gqwkd1hY9JFWVMqvBRWVZ8pYqRlHk9tNQHaUG7CipVihwtIBpj7gbuhlTJJZ/XuHLhNK5c6M4hvUopVcxy+b20HZiVdb8p/diQ14iID6ghNTmqlFLKIbkE9BeBeSIyW0T8wE3A+lOuWQ/ckr79AeA3haifK6WUGt6oJZd0TfxWYCOpZYv3GmN2isidwBZjzHrgv4AfisheoBMy82xKKaUcklMN3RizAdhwymO3Z90eBD5o79CUUkqNha7tUkqpCUIDulJKTRAa0JVSaoLQgK6UUhOEa90WReQY8Gaen17PKbtQzwD6NZ8Z9Gs+M4znaz7LGNMw1BOuBfTxEJEtwzWnmaj0az4z6Nd8ZijU16wlF6WUmiA0oCul1ARRqgH9brcH4AL9ms8M+jWfGQryNZdkDV0ppdTpSjVDV0opdQoN6EopNUGUXEAXkdUisltE9orIbW6Pp9BEZJaIPC0iu0Rkp4h81u0xOUFEvCLysog85vZYnCAik0XkpyLymoi0isglbo+p0ETkf6b/Te8QkR+JSIXbY7KbiNwrIkdFZEfWY3Ui8isR2ZP+s9au9yupgJ4+sPou4GpgIXCziCx0d1QFFwc+b4xZCKwAPn0GfM0AnwVa3R6Eg/4DeMIYcy6wjAn+tYtII/AZYLkxZjGp1twTse3294HVpzx2G/CUMWYe8FT6vi1KKqCTdWC1MSYKWAdWT1jGmA5jzNb07T5S/9Eb3R1VYYlIE/A+4B63x+IEEakB3k3qXAGMMVFjTLerg3KGD6hMn3IWAA65PB7bGWOeIXVGRLbrgfvSt+8D/tiu9yu1gD7UgdUTOrhlE5EW4B3AZpeHUmhfB/4WSLo8DqfMBo4B30uXme4RkQl9Urcxph34V+AtoAPoMcY86e6oHDPNGNORvn0YsO2Q5FIL6GcsEakCfgb8tTGm1+3xFIqIXAscNca85PZYHOQDzge+bYx5BxDCxl/Di1G6bnw9qR9mM4GgiHzE3VE5L31Up21rx0stoOdyYPWEIyJlpIL5/caYh90eT4GtBNaIyAFSJbXLReS/3R1SwbUBbcYY6zevn5IK8BPZFcAbxphjxpgY8DDwTpfH5JQjIjIDIP3nUbteuNQCei4HVk8oIiKkaqutxph/d3s8hWaM+aIxpskY00Lq7/c3xpgJnbkZYw4DB0VkfvqhVcAuF4fkhLeAFSISSP8bX8UEnwjOsh64JX37FuARu144pzNFi8VwB1a7PKxCWwn8GbBdRLalH/u79DmvauL4H8D96URlP/Axl8dTUMaYzSLyU2ArqZVcLzMBWwCIyI+Ay4B6EWkD7gC+CjwkIn9BqoX4n9r2frr1XymlJoZSK7kopZQahgZ0pZSaIDSgK6XUBKEBXSmlJggN6EopNUFoQFdKqQlCA7pSSk0Q/x8SQL+1hCQi2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# k-space for plotting\n",
    "kmin = 0\n",
    "kmax = 10\n",
    "ks = np.linspace(kmin, kmax, k_samples)\n",
    "\n",
    "which_row = 10000\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ks, X[which_row], label=f'{y[which_row]+3} mirrors')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b359c3",
   "metadata": {},
   "source": [
    "We will now split the dataset randomly into the training and test parts, borrowing a function from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10e6276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 3-mirror elements in the testing set: 0.0018492804028186927\n",
      "Proportion of 3-mirror elements in the testing set: 0.00184928440263788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify = y, \n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# the 'stratify = y' argument ensures that the training and testing sets contain the same proportion of each class\n",
    "# we especially need to watch out for this for the lowest number of mirrors \n",
    "# which forms a tiny proportion of the whole dataset:\n",
    "\n",
    "print('Proportion of 3-mirror elements in the testing set:', list(y_test).count(0) / len(y_test))\n",
    "print('Proportion of 3-mirror elements in the testing set:', list(y_train).count(0) / len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c463f07",
   "metadata": {},
   "source": [
    "Checks out. Let us build the network now (with the sizes of hidden layers chosen arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccce245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=k_samples, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "\n",
    "# The last layer consists of 8 nodes as we generated the data for 3 to 10 mirrors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b777487",
   "metadata": {},
   "source": [
    "We shall define metrics to gauge the performance of our model using a useful function from the sklearn module. To have it evaluated after each epoch of the training, we will wrap it in a Keras callback. This callback will evaluate the classification report and log it to a log dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fbcc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report_log_dict = {f'{class_id}' : {'precision': [], 'recall': [], 'f1-score': []} \n",
    "                                  for class_id in range(3, 11)} | {'accuracy': [], \n",
    "                                        'macro avg': {'precision': [], 'recall': [], 'f1-score': []},\n",
    "                                        'weighted avg': {'precision': [], 'recall': [], 'f1-score': []}}\n",
    "\n",
    "class PerformanceEvaluationCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, x_test, y_test):\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Evaluating the model...')\n",
    "        y_pred = self.model.predict(self.x_test, verbose=0, batch_size = len(self.x_test))\n",
    "        y_pred = np.argmax(y_pred, axis = 1)\n",
    "        report = classification_report(self.y_test, y_pred, \n",
    "                              target_names = ['3','4','5','6','7','8','9','10'], output_dict = True)\n",
    "        for class_id in classes + ['macro avg', 'weighted avg']:\n",
    "            classification_report_log_dict[class_id]['precision'].append(report[class_id]['precision'])\n",
    "            classification_report_log_dict[class_id]['recall'].append(report[class_id]['recall'])\n",
    "            classification_report_log_dict[class_id]['f1-score'].append(report[class_id]['f1-score'])\n",
    "        \n",
    "        classification_report_log_dict['accuracy'].append(report['accuracy'])\n",
    "        \n",
    "        \n",
    "performance_callback = PerformanceEvaluationCallback(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ae665",
   "metadata": {},
   "source": [
    "Now we compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151845f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', run_eagerly = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637265f3",
   "metadata": {},
   "source": [
    "Set necessary prerequisites and train, using the testing data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7225/7225 [==============================] - 78s 11ms/step - loss: 1.2012 - val_loss: 1.0555\n",
      "Evaluating the model...\n",
      "Epoch 2/200\n",
      "7225/7225 [==============================] - 79s 11ms/step - loss: 0.9823 - val_loss: 0.9143\n",
      "Evaluating the model...\n",
      "Epoch 3/200\n",
      "7225/7225 [==============================] - 93s 13ms/step - loss: 0.8622 - val_loss: 0.8069\n",
      "Evaluating the model...\n",
      "Epoch 4/200\n",
      "7225/7225 [==============================] - 91s 13ms/step - loss: 0.7637 - val_loss: 0.7305\n",
      "Evaluating the model...\n",
      "Epoch 5/200\n",
      "7225/7225 [==============================] - 93s 13ms/step - loss: 0.6793 - val_loss: 0.6357\n",
      "Evaluating the model...\n",
      "Epoch 6/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.6121 - val_loss: 0.5769\n",
      "Evaluating the model...\n",
      "Epoch 7/200\n",
      "7225/7225 [==============================] - 92s 13ms/step - loss: 0.5544 - val_loss: 0.5191\n",
      "Evaluating the model...\n",
      "Epoch 8/200\n",
      "7225/7225 [==============================] - 92s 13ms/step - loss: 0.5034 - val_loss: 0.4747\n",
      "Evaluating the model...\n",
      "Epoch 9/200\n",
      "7225/7225 [==============================] - 91s 13ms/step - loss: 0.4605 - val_loss: 0.4808\n",
      "Evaluating the model...\n",
      "Epoch 10/200\n",
      "7225/7225 [==============================] - 93s 13ms/step - loss: 0.4251 - val_loss: 0.4013\n",
      "Evaluating the model...\n",
      "Epoch 11/200\n",
      "7225/7225 [==============================] - 91s 13ms/step - loss: 0.3963 - val_loss: 0.3697\n",
      "Evaluating the model...\n",
      "Epoch 12/200\n",
      "7225/7225 [==============================] - 92s 13ms/step - loss: 0.3709 - val_loss: 0.3443\n",
      "Evaluating the model...\n",
      "Epoch 13/200\n",
      "7225/7225 [==============================] - 90s 12ms/step - loss: 0.3493 - val_loss: 0.3318\n",
      "Evaluating the model...\n",
      "Epoch 14/200\n",
      "7225/7225 [==============================] - 89s 12ms/step - loss: 0.3300 - val_loss: 0.3255\n",
      "Evaluating the model...\n",
      "Epoch 15/200\n",
      "7225/7225 [==============================] - 86s 12ms/step - loss: 0.3135 - val_loss: 0.2942\n",
      "Evaluating the model...\n",
      "Epoch 16/200\n",
      "7225/7225 [==============================] - 87s 12ms/step - loss: 0.2981 - val_loss: 0.3382\n",
      "Evaluating the model...\n",
      "Epoch 17/200\n",
      "7225/7225 [==============================] - 83s 11ms/step - loss: 0.2814 - val_loss: 0.2555\n",
      "Evaluating the model...\n",
      "Epoch 18/200\n",
      "7225/7225 [==============================] - 84s 12ms/step - loss: 0.2672 - val_loss: 0.3034\n",
      "Evaluating the model...\n",
      "Epoch 19/200\n",
      "7225/7225 [==============================] - 83s 11ms/step - loss: 0.2552 - val_loss: 0.2193\n",
      "Evaluating the model...\n",
      "Epoch 20/200\n",
      "7225/7225 [==============================] - 82s 11ms/step - loss: 0.2452 - val_loss: 0.2284\n",
      "Evaluating the model...\n",
      "Epoch 21/200\n",
      "7225/7225 [==============================] - 82s 11ms/step - loss: 0.2329 - val_loss: 0.2276\n",
      "Evaluating the model...\n",
      "Epoch 22/200\n",
      "7225/7225 [==============================] - 88s 12ms/step - loss: 0.2236 - val_loss: 0.2492\n",
      "Evaluating the model...\n",
      "Epoch 23/200\n",
      "7225/7225 [==============================] - 81s 11ms/step - loss: 0.2145 - val_loss: 0.2287\n",
      "Evaluating the model...\n",
      "Epoch 24/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.2056 - val_loss: 0.1774\n",
      "Evaluating the model...\n",
      "Epoch 25/200\n",
      "7225/7225 [==============================] - 71s 10ms/step - loss: 0.1976 - val_loss: 0.1794\n",
      "Evaluating the model...\n",
      "Epoch 26/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.1920 - val_loss: 0.1624\n",
      "Evaluating the model...\n",
      "Epoch 27/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.1821 - val_loss: 0.1544\n",
      "Evaluating the model...\n",
      "Epoch 28/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.1782 - val_loss: 0.2652\n",
      "Evaluating the model...\n",
      "Epoch 29/200\n",
      "7225/7225 [==============================] - 70s 10ms/step - loss: 0.1693 - val_loss: 0.1566\n",
      "Evaluating the model...\n",
      "Epoch 30/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1674 - val_loss: 0.1510\n",
      "Evaluating the model...\n",
      "Epoch 31/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1588 - val_loss: 0.1399\n",
      "Evaluating the model...\n",
      "Epoch 32/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1586 - val_loss: 0.1986\n",
      "Evaluating the model...\n",
      "Epoch 33/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1516 - val_loss: 0.1715\n",
      "Evaluating the model...\n",
      "Epoch 34/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1490 - val_loss: 0.1439\n",
      "Evaluating the model...\n",
      "Epoch 35/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1468 - val_loss: 0.1744\n",
      "Evaluating the model...\n",
      "Epoch 36/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1432 - val_loss: 0.1701\n",
      "Evaluating the model...\n",
      "Epoch 37/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1395 - val_loss: 0.1464\n",
      "Evaluating the model...\n",
      "Epoch 38/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1357 - val_loss: 0.1224\n",
      "Evaluating the model...\n",
      "Epoch 39/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1345 - val_loss: 0.2559\n",
      "Evaluating the model...\n",
      "Epoch 40/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1320 - val_loss: 0.1116\n",
      "Evaluating the model...\n",
      "Epoch 41/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1285 - val_loss: 0.0952\n",
      "Evaluating the model...\n",
      "Epoch 42/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1254 - val_loss: 0.1735\n",
      "Evaluating the model...\n",
      "Epoch 43/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1233 - val_loss: 0.1267\n",
      "Evaluating the model...\n",
      "Epoch 44/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.1267 - val_loss: 0.1140\n",
      "Evaluating the model...\n",
      "Epoch 45/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1214 - val_loss: 0.0949\n",
      "Evaluating the model...\n",
      "Epoch 46/200\n",
      "7225/7225 [==============================] - 66s 9ms/step - loss: 0.1209 - val_loss: 0.0909\n",
      "Evaluating the model...\n",
      "Epoch 47/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1163 - val_loss: 0.1129\n",
      "Evaluating the model...\n",
      "Epoch 48/200\n",
      "7225/7225 [==============================] - 66s 9ms/step - loss: 0.1154 - val_loss: 0.1217\n",
      "Evaluating the model...\n",
      "Epoch 49/200\n",
      "7225/7225 [==============================] - 67s 9ms/step - loss: 0.1123 - val_loss: 0.1082\n",
      "Evaluating the model...\n",
      "Epoch 50/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1116 - val_loss: 0.1681\n",
      "Evaluating the model...\n",
      "Epoch 51/200\n",
      "7225/7225 [==============================] - 98s 14ms/step - loss: 0.1097 - val_loss: 0.0811\n",
      "Evaluating the model...\n",
      "Epoch 52/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1101 - val_loss: 0.1567\n",
      "Evaluating the model...\n",
      "Epoch 53/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1066 - val_loss: 0.1530\n",
      "Evaluating the model...\n",
      "Epoch 54/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1052 - val_loss: 0.1115\n",
      "Evaluating the model...\n",
      "Epoch 55/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1043 - val_loss: 0.0641\n",
      "Evaluating the model...\n",
      "Epoch 56/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1029 - val_loss: 0.1047\n",
      "Evaluating the model...\n",
      "Epoch 57/200\n",
      "7225/7225 [==============================] - 96s 13ms/step - loss: 0.1004 - val_loss: 0.0956\n",
      "Evaluating the model...\n",
      "Epoch 58/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1009 - val_loss: 0.1031\n",
      "Evaluating the model...\n",
      "Epoch 59/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.1020 - val_loss: 0.1680\n",
      "Evaluating the model...\n",
      "Epoch 60/200\n",
      "7225/7225 [==============================] - 96s 13ms/step - loss: 0.0970 - val_loss: 0.2127\n",
      "Evaluating the model...\n",
      "Epoch 61/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.0971 - val_loss: 0.3410\n",
      "Evaluating the model...\n",
      "Epoch 62/200\n",
      "7225/7225 [==============================] - 97s 13ms/step - loss: 0.0972 - val_loss: 0.0700\n",
      "Evaluating the model...\n",
      "Epoch 63/200\n",
      "7225/7225 [==============================] - 80s 11ms/step - loss: 0.0949 - val_loss: 0.0655\n",
      "Evaluating the model...\n",
      "Epoch 64/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.0912 - val_loss: 0.0924\n",
      "Evaluating the model...\n",
      "Epoch 65/200\n",
      "7225/7225 [==============================] - 72s 10ms/step - loss: 0.0915 - val_loss: 0.0713\n",
      "Evaluating the model...\n",
      "Epoch 66/200\n",
      "7225/7225 [==============================] - 72s 10ms/step - loss: 0.0907 - val_loss: 0.1220\n",
      "Evaluating the model...\n",
      "Epoch 67/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.0949 - val_loss: 0.0672\n",
      "Evaluating the model...\n",
      "Epoch 68/200\n",
      "7225/7225 [==============================] - 72s 10ms/step - loss: 0.0889 - val_loss: 0.0775\n",
      "Evaluating the model...\n",
      "Epoch 69/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.0899 - val_loss: 0.0509\n",
      "Evaluating the model...\n",
      "Epoch 70/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.0874 - val_loss: 0.0886\n",
      "Evaluating the model...\n",
      "Epoch 71/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.0853 - val_loss: 0.1233\n",
      "Evaluating the model...\n",
      "Epoch 72/200\n",
      "7225/7225 [==============================] - 54s 7ms/step - loss: 0.0852 - val_loss: 0.0555\n",
      "Evaluating the model...\n",
      "Epoch 73/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0859 - val_loss: 0.0604\n",
      "Evaluating the model...\n",
      "Epoch 74/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0838 - val_loss: 0.0858\n",
      "Evaluating the model...\n",
      "Epoch 75/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0809 - val_loss: 0.0609\n",
      "Evaluating the model...\n",
      "Epoch 76/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0800 - val_loss: 0.0555\n",
      "Evaluating the model...\n",
      "Epoch 77/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0774 - val_loss: 0.1138\n",
      "Evaluating the model...\n",
      "Epoch 78/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0782 - val_loss: 0.0681\n",
      "Evaluating the model...\n",
      "Epoch 79/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0808 - val_loss: 0.0561\n",
      "Evaluating the model...\n",
      "Epoch 80/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0784 - val_loss: 0.0466\n",
      "Evaluating the model...\n",
      "Epoch 81/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0758 - val_loss: 0.0589\n",
      "Evaluating the model...\n",
      "Epoch 82/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0769 - val_loss: 0.0723\n",
      "Evaluating the model...\n",
      "Epoch 83/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0761 - val_loss: 0.0539\n",
      "Evaluating the model...\n",
      "Epoch 84/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0761 - val_loss: 0.0658\n",
      "Evaluating the model...\n",
      "Epoch 85/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0767 - val_loss: 0.1298\n",
      "Evaluating the model...\n",
      "Epoch 86/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0750 - val_loss: 0.0528\n",
      "Evaluating the model...\n",
      "Epoch 87/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0713 - val_loss: 0.0537\n",
      "Evaluating the model...\n",
      "Epoch 88/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0733 - val_loss: 0.0930\n",
      "Evaluating the model...\n",
      "Epoch 89/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0721 - val_loss: 0.0618\n",
      "Evaluating the model...\n",
      "Epoch 90/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0738 - val_loss: 0.0603\n",
      "Evaluating the model...\n",
      "Epoch 91/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0704 - val_loss: 0.0394\n",
      "Evaluating the model...\n",
      "Epoch 92/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0696 - val_loss: 0.0793\n",
      "Evaluating the model...\n",
      "Epoch 93/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0687 - val_loss: 0.0529\n",
      "Evaluating the model...\n",
      "Epoch 94/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0690 - val_loss: 0.0352\n",
      "Evaluating the model...\n",
      "Epoch 95/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0701 - val_loss: 0.0370\n",
      "Evaluating the model...\n",
      "Epoch 96/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0680 - val_loss: 0.0413\n",
      "Evaluating the model...\n",
      "Epoch 97/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0692 - val_loss: 0.0463\n",
      "Evaluating the model...\n",
      "Epoch 98/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0678 - val_loss: 0.0664\n",
      "Evaluating the model...\n",
      "Epoch 99/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0660 - val_loss: 0.0583\n",
      "Evaluating the model...\n",
      "Epoch 100/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0668 - val_loss: 0.0557\n",
      "Evaluating the model...\n",
      "Epoch 101/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0646 - val_loss: 0.0606\n",
      "Evaluating the model...\n",
      "Epoch 102/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0643 - val_loss: 0.0472\n",
      "Evaluating the model...\n",
      "Epoch 103/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0628 - val_loss: 0.0668\n",
      "Evaluating the model...\n",
      "Epoch 104/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0655 - val_loss: 0.1012\n",
      "Evaluating the model...\n",
      "Epoch 105/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0637 - val_loss: 0.0345\n",
      "Evaluating the model...\n",
      "Epoch 106/200\n",
      "7225/7225 [==============================] - 50s 7ms/step - loss: 0.0629 - val_loss: 0.0400\n",
      "Evaluating the model...\n",
      "Epoch 107/200\n",
      "7225/7225 [==============================] - 52s 7ms/step - loss: 0.0635 - val_loss: 0.0497\n",
      "Evaluating the model...\n",
      "Epoch 108/200\n",
      "7225/7225 [==============================] - 51s 7ms/step - loss: 0.0624 - val_loss: 0.0802\n",
      "Evaluating the model...\n",
      "Epoch 109/200\n",
      "7225/7225 [==============================] - 52s 7ms/step - loss: 0.0645 - val_loss: 0.0477\n",
      "Evaluating the model...\n",
      "Epoch 110/200\n",
      "7225/7225 [==============================] - 57s 8ms/step - loss: 0.0615 - val_loss: 0.0537\n",
      "Evaluating the model...\n",
      "Epoch 111/200\n",
      "7225/7225 [==============================] - 65s 9ms/step - loss: 0.0634 - val_loss: 0.0374\n",
      "Evaluating the model...\n",
      "Epoch 112/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.0584 - val_loss: 0.0947\n",
      "Evaluating the model...\n",
      "Epoch 113/200\n",
      "7225/7225 [==============================] - 70s 10ms/step - loss: 0.0583 - val_loss: 0.1425\n",
      "Evaluating the model...\n",
      "Epoch 114/200\n",
      "7225/7225 [==============================] - 74s 10ms/step - loss: 0.0598 - val_loss: 0.0537\n",
      "Evaluating the model...\n",
      "Epoch 115/200\n",
      "7225/7225 [==============================] - 74s 10ms/step - loss: 0.0613 - val_loss: 0.0387\n",
      "Evaluating the model...\n",
      "Epoch 116/200\n",
      "7225/7225 [==============================] - 74s 10ms/step - loss: 0.0614 - val_loss: 0.0429\n",
      "Evaluating the model...\n",
      "Epoch 117/200\n",
      "7225/7225 [==============================] - 83s 11ms/step - loss: 0.0575 - val_loss: 0.0258\n",
      "Evaluating the model...\n",
      "Epoch 118/200\n",
      "7225/7225 [==============================] - 75s 10ms/step - loss: 0.0587 - val_loss: 0.0442\n",
      "Evaluating the model...\n",
      "Epoch 119/200\n",
      "7225/7225 [==============================] - 78s 11ms/step - loss: 0.0603 - val_loss: 0.0437\n",
      "Evaluating the model...\n",
      "Epoch 120/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.0585 - val_loss: 0.0302\n",
      "Evaluating the model...\n",
      "Epoch 121/200\n",
      "7225/7225 [==============================] - 74s 10ms/step - loss: 0.0566 - val_loss: 0.0355\n",
      "Evaluating the model...\n",
      "Epoch 122/200\n",
      "7225/7225 [==============================] - 70s 10ms/step - loss: 0.0569 - val_loss: 0.0513\n",
      "Evaluating the model...\n",
      "Epoch 123/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.0581 - val_loss: 0.0574\n",
      "Evaluating the model...\n",
      "Epoch 124/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.0544 - val_loss: 0.0272\n",
      "Evaluating the model...\n",
      "Epoch 125/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.0554 - val_loss: 0.0308\n",
      "Evaluating the model...\n",
      "Epoch 126/200\n",
      "7225/7225 [==============================] - 72s 10ms/step - loss: 0.0545 - val_loss: 0.0432\n",
      "Evaluating the model...\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7225/7225 [==============================] - 69s 9ms/step - loss: 0.0540 - val_loss: 0.0400\n",
      "Evaluating the model...\n",
      "Epoch 128/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.0546 - val_loss: 0.0283\n",
      "Evaluating the model...\n",
      "Epoch 129/200\n",
      "7225/7225 [==============================] - 68s 9ms/step - loss: 0.0532 - val_loss: 0.0396\n",
      "Evaluating the model...\n",
      "Epoch 130/200\n",
      "7225/7225 [==============================] - 69s 10ms/step - loss: 0.0550 - val_loss: 0.0439\n",
      "Evaluating the model...\n",
      "Epoch 131/200\n",
      "7225/7225 [==============================] - 69s 9ms/step - loss: 0.0546 - val_loss: 0.0390\n",
      "Evaluating the model...\n",
      "Epoch 132/200\n",
      "7225/7225 [==============================] - 73s 10ms/step - loss: 0.0533 - val_loss: 0.0741\n",
      "Evaluating the model...\n",
      "Epoch 133/200\n",
      "7225/7225 [==============================] - 75s 10ms/step - loss: 0.0506 - val_loss: 0.0386\n",
      "Evaluating the model...\n",
      "Epoch 134/200\n",
      "1755/7225 [======>.......................] - ETA: 47s - loss: 0.0526"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "callbacks = [\n",
    "    performance_callback,\n",
    "    \n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1247e8d",
   "metadata": {},
   "source": [
    "The best model is saved and can be recovered with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606eb7",
   "metadata": {},
   "source": [
    "Let us now plot the history of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc383fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "cmap = cm.get_cmap('jet')\n",
    "colors = [cmap(i/7.) for i in range(8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a680f",
   "metadata": {},
   "source": [
    "First, the overall accuracy and averages of the class-dependent metrics: precision, recall and the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1548e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(12,8))\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "ax1.plot(classification_report_log_dict['accuracy'], label = 'Accuracy', linewidth = 3)\n",
    "\n",
    "ax1.plot(classification_report_log_dict['macro avg']['precision'], \n",
    "         label = 'Macro avg: precision', linestyle='--', linewidth = 0.8)\n",
    "ax1.plot(classification_report_log_dict['macro avg']['recall'], \n",
    "         label = 'Macro avg: recall', linestyle='--', linewidth = 0.8)\n",
    "ax1.plot(classification_report_log_dict['macro avg']['f1-score'], \n",
    "         label = 'Macro avg: f1-score', linestyle='--', linewidth = 0.8)\n",
    "\n",
    "ax1.plot(classification_report_log_dict['weighted avg']['precision'], \n",
    "         label = 'Weighted avg: precision', linestyle='-.', linewidth = 0.8)\n",
    "ax1.plot(classification_report_log_dict['weighted avg']['recall'], \n",
    "         label = 'Weighted avg: recall', linestyle='-.', linewidth = 0.8)\n",
    "ax1.plot(classification_report_log_dict['weighted avg']['f1-score'], \n",
    "         label = 'Weighted avg: f1-score', linestyle='-.', linewidth = 0.8)\n",
    "\n",
    "\n",
    "ax1.legend(loc='best')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d39b8",
   "metadata": {},
   "source": [
    "Then precision for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8096fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(12,8))\n",
    "\n",
    "metric = \"Precision\"\n",
    "ax1.set_ylabel(metric)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "for class_id, col in zip(classes, colors):\n",
    "    class_metric = metric + \": \" + class_id + \" mirrors\"\n",
    "    ax1.plot(classification_report_log_dict[class_id]['precision'], \n",
    "             label = class_id + ' mirrors', color=col)\n",
    "    \n",
    "ax1.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7244e1",
   "metadata": {},
   "source": [
    "Recall for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(12,8))\n",
    "\n",
    "metric = \"Recall\"\n",
    "ax1.set_ylabel(metric)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "for class_id, col in zip(classes, colors):\n",
    "    class_metric = metric + \": \" + class_id + \" mirrors\"\n",
    "    ax1.plot(classification_report_log_dict[class_id]['recall'], \n",
    "             label = class_id + ' mirrors', color=col)\n",
    "    \n",
    "ax1.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d26ad5",
   "metadata": {},
   "source": [
    "And the F1 score for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b83cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(12,8))\n",
    "\n",
    "metric = \"F1-score\"\n",
    "ax1.set_ylabel(metric)\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "for class_id, col in zip(classes, colors):\n",
    "    class_metric = metric + \": \" + class_id + \" mirrors\"\n",
    "    ax1.plot(classification_report_log_dict[class_id]['f1-score'], \n",
    "             label = class_id + ' mirrors', color=col)\n",
    "    \n",
    "ax1.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d67d142",
   "metadata": {},
   "source": [
    "For completeness and convenience, we also output the final classification report in text form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=len(X_test), verbose=1)\n",
    "y_test_check = y_test\n",
    "y_pred_check = np.argmax(y_pred, axis = 1)\n",
    "print(classification_report(y_test_check, y_pred_check, target_names = classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4c9be",
   "metadata": {},
   "source": [
    "History for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9864c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_log_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
